{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.4.1 神经网络的构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, **kwargs):\n",
    "    super(MLP, self).__init__()\n",
    "    self.hidden = nn.Linear(784, 256)\n",
    "    self.act = nn.ReLU()\n",
    "    self.output = nn.Linear(256, 10)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    a = self.act(self.hidden(x))\n",
    "    return self.output(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0658,  0.1477,  0.0087, -0.1591, -0.0906,  0.1742,  0.0189,  0.1231,\n",
       "          0.0275, -0.0591],\n",
       "        [-0.0126,  0.0261,  0.0054, -0.0588, -0.0947,  0.1191,  0.0598, -0.0363,\n",
       "          0.1346, -0.0306]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2, 784)\n",
    "net = MLP()\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 神经网络中常见的层\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 不含模型参数的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MyLayer(nn.Module):\n",
    "  def __init__(self, **kwargs):\n",
    "    super(MyLayer, self).__init__(**kwargs)\n",
    "  def forward(self, x):\n",
    "    return x - x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = MyLayer()\n",
    "layer(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 含模型参数的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyListDense(\n",
      "  (params): ParameterList(\n",
      "      (0): Parameter containing: [torch.float32 of size 4x4]\n",
      "      (1): Parameter containing: [torch.float32 of size 4x4]\n",
      "      (2): Parameter containing: [torch.float32 of size 4x4]\n",
      "      (3): Parameter containing: [torch.float32 of size 4x1]\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-70.6800],\n",
       "        [  8.7415],\n",
       "        [ -3.3696],\n",
       "        [ -0.9187]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyListDense(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyListDense, self).__init__()\n",
    "    self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])\n",
    "    self.params.append(nn.Parameter(torch.randn(4, 1)))\n",
    "    \n",
    "  def forward(self, x):\n",
    "    for i in range(len(self.params)):\n",
    "      x = torch.mm(x, self.params[i])\n",
    "    return x\n",
    "  \n",
    "net = MyListDense()\n",
    "print(net)\n",
    "net(torch.randn(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDictDense(\n",
      "  (params): ParameterDict(\n",
      "      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (linear2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (linear3): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2645],\n",
       "        [ 1.2620],\n",
       "        [ 3.0418],\n",
       "        [-0.0383]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDictDense(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyDictDense, self).__init__()\n",
    "    self.params = nn.ParameterDict({\n",
    "      'linear1': nn.Parameter(torch.randn(4, 4)),\n",
    "      'linear2': nn.Parameter(torch.randn(4, 4)),\n",
    "    })\n",
    "    self.params.update({'linear3': nn.Parameter(torch.randn(4, 1))})\n",
    "  \n",
    "  def forward(self, x, choice='linear1'):\n",
    "    return torch.mm(x, self.params[choice])\n",
    "  \n",
    "net = MyDictDense()\n",
    "print(net)\n",
    "net(torch.randn(4,4), \"linear3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def corr2d(X, K):\n",
    "  h, w = K.shape\n",
    "  X, K = X.float(), K.float()\n",
    "  Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "  for i in range(Y.shape[0]):\n",
    "    for j in range(Y.shape[1]):\n",
    "      Y[i, j] = (X[i:i+h, j:j+w] * K).sum()\n",
    "  return Y\n",
    "\n",
    "class Conv2D(nn.Module):\n",
    "  def __init__(self, kernel_size):\n",
    "    super(Conv2D, self).__init__()\n",
    "    self.weight = nn.Parameter(torch.randn(kernel_size))\n",
    "    self.bias = nn.Parameter(torch.randn(1))\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return corr2d(x, self.weight) + self.bias\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def comp_conv2d(conv2d, X):\n",
    "  X = X.view((1, 1) + X.shape)\n",
    "  Y = conv2d(X)\n",
    "  return Y.view(Y.shape[2:])\n",
    "\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)\n",
    "X = torch.rand(8, 8)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def pool2d(X, pool_size, mode=\"max\"):\n",
    "  p_h, p_w = pool_size\n",
    "  Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "  for i in range(Y.shape[0]):\n",
    "    for j in range(Y.shape[1]):\n",
    "      if mode == \"max\":\n",
    "        Y[i, j] = X[i:i+p_h, j:j+p_w].max()\n",
    "      elif mode == \"avg\":\n",
    "        Y[i, j] = X[i:i+p_h, j:j+p_w].mean()        \n",
    "  return Y\n",
    "\n",
    "X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]], dtype=torch.float)\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3 模型示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    #输入图像channel：1，输出channel：6，kernel_size：5*5卷积核\n",
    "    self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "    # an affine operation: y = Wx + b\n",
    "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "    self.fc2 = nn.Linear(120, 84)\n",
    "    self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    # 2x2 Max pooling\n",
    "    x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "    # 如果是方阵，则可以只是用一个数字进行定义\n",
    "    x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "    x = x.view(-1, self.num_flat_features(x))\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return x\n",
    "  \n",
    "  def num_flat_features(self, x):\n",
    "    size = x.size()[1:]\n",
    "    num_features = 1\n",
    "    for s in size:\n",
    "      num_features *= s\n",
    "    return num_features\n",
    "  \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "for i in range(len(params)):\n",
    "  print(params[i].size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0353,  0.0340, -0.0714, -0.0403, -0.0404, -0.0961, -0.0351, -0.0009,\n",
      "          0.0413,  0.1175]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=6400, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(AlexNet, self).__init__()\n",
    "    self.conv = nn.Sequential(\n",
    "      nn.Conv2d(1, 96, 11, 4),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(3,2),\n",
    "      nn.Conv2d(96, 256, 5, 1, 2),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(3, 2),\n",
    "      \n",
    "      nn.Conv2d(256, 384, 3, 1, 1),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(384, 384, 3, 1, 1),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(384, 256, 3, 1, 1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(3, 2)\n",
    "    )\n",
    "    self.fc = nn.Sequential(\n",
    "      nn.Linear(256*5*5, 4096),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(0.5),\n",
    "      nn.Linear(4096, 4096),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(0.5),\n",
    "      nn.Linear(4096, 10)\n",
    "    )\n",
    "  \n",
    "  def forward(self, img):\n",
    "    feature = self.conv(img)\n",
    "    output = self.fc(feature.view(img.shape[0], -1))\n",
    "    return output\n",
    "  \n",
    "net = AlexNet()\n",
    "print(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nngen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
